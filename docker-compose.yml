version: "3.7"
services:


  namenode:
    container_name: namenode
    image: "docker.io/dockeralexandrtalan/hadoop"
    entrypoint: /bin/bash entrypoint.sh --command "hdfs start namenode" --config "/usr/local/lib/hadoop-2.9.2/etc/hadoop/hadoop-config.json"
    restart: always
    volumes:
      - ./hadoop-config:/usr/local/lib/hadoop-2.9.2/etc/hadoop/
      - ./hadoop-logs:/usr/local/lib/hadoop-2.9.2/logs/
    ports:
      - "9870:9870"
    networks:
      - hadoop
  datanode:
    container_name: datanode1
    image: "docker.io/dockeralexandrtalan/hadoop"
    entrypoint: /bin/bash entrypoint.sh --command "hdfs start datanode" --config "/usr/local/lib/hadoop-2.9.2/etc/hadoop/hadoop-config.json"
    restart: always
    volumes:
      - ./hadoop-config:/usr/local/lib/hadoop-2.9.2/etc/hadoop/
      - ./hadoop-logs:/usr/local/lib/hadoop-2.9.2/logs/
    ports:
      - "50075:50075"
    networks:
      - hadoop
    depends_on:
      - namenode
  datanode2:
    container_name: datanode2
    image: "docker.io/dockeralexandrtalan/hadoop"
    entrypoint: /bin/bash entrypoint.sh --command "hdfs start datanode" --config "/usr/local/lib/hadoop-2.9.2/etc/hadoop/hadoop-config.json"
    restart: always
    volumes:
      - ./hadoop-config:/usr/local/lib/hadoop-2.9.2/etc/hadoop/
      - ./hadoop-logs:/usr/local/lib/hadoop-2.9.2/logs/
    ports:
      - "50076:50075"
    networks:
      - hadoop
    depends_on:
      - namenode


  resourcemanager:
    container_name: resourcemanager
    image: "docker.io/dockeralexandrtalan/hadoop"
    entrypoint: /bin/bash entrypoint.sh --command "yarn start resourcemanager" --config "/usr/local/lib/hadoop-2.9.2/etc/hadoop/hadoop-config.json"
    restart: always
    volumes:
      - ./hadoop-config:/usr/local/lib/hadoop-2.9.2/etc/hadoop/
      - ./hadoop-logs:/usr/local/lib/hadoop-2.9.2/logs/
    ports:
      - "12050:12050"
    networks:
      - hadoop
  nodemanager1:
    container_name: nodemanager1
    image: "docker.io/dockeralexandrtalan/hadoop"
    entrypoint: /bin/bash entrypoint.sh --command "yarn start nodemanager" --config "/usr/local/lib/hadoop-2.9.2/etc/hadoop/hadoop-config.json"
    restart: always
    volumes:
      - ./hadoop-config:/usr/local/lib/hadoop-2.9.2/etc/hadoop/
      - ./hadoop-logs:/usr/local/lib/hadoop-2.9.2/logs/
    ports:
      - "8042:8042"
    networks:
      - hadoop
    depends_on:
      - resourcemanager
  nodemanager2:
    container_name: nodemanager2
    image: "docker.io/dockeralexandrtalan/hadoop"
    entrypoint: /bin/bash entrypoint.sh --command "yarn start nodemanager" --config "/usr/local/lib/hadoop-2.9.2/etc/hadoop/hadoop-config.json"
    restart: always
    volumes:
      - ./hadoop-config:/usr/local/lib/hadoop-2.9.2/etc/hadoop/
      - ./hadoop-logs:/usr/local/lib/hadoop-2.9.2/logs/
    ports:
      - "8043:8042"
    networks:
      - hadoop
    depends_on:
      - resourcemanager


  zookeeper1:
    container_name: zookeeper1
    entrypoint: /bin/bash run-zookeeper.sh --myid 1
    image: "docker.io/dockeralexandrtalan/zookeeper"
    restart: always
    volumes:
      - ./zookeeper-config:/usr/local/lib/apache-zookeeper-3.5.6-bin/conf
    ports:
      - "2181:2181"
    networks:
      - hadoop
  zookeeper2:
    container_name: zookeeper2
    entrypoint: /bin/bash run-zookeeper.sh --myid 2
    image: "docker.io/dockeralexandrtalan/zookeeper"
    restart: always
    volumes:
      - ./zookeeper-config:/usr/local/lib/apache-zookeeper-3.5.6-bin/conf
    ports:
      - "2182:2181"
    networks:
      - hadoop
  zookeeper3:
    container_name: zookeeper3
    entrypoint: /bin/bash run-zookeeper.sh --myid 3
    image: "docker.io/dockeralexandrtalan/zookeeper"
    restart: always
    volumes:
      - ./zookeeper-config:/usr/local/lib/apache-zookeeper-3.5.6-bin/conf
    ports:
      - "2183:2181"
    networks:
      - hadoop


  hbase_master:
    container_name: hbase_master
    image: "docker.io/dockeralexandrtalan/hbase"
    entrypoint: /bin/bash entrypoint.sh --command "master" --config "/root/hbase-2.2.2/conf/hbase-config.json"
    restart: always
    volumes:
      - ./hbase-config:/root/hbase-2.2.2/conf
      - ./ssh_hbase:/root/.ssh/hbase
    ports:
      - "16010:16010"
    depends_on:
      - zookeeper1
      - namenode
    networks:
      - hadoop
  hbase_region_server_1:
    container_name: hbase_region_server_1
    image: "docker.io/dockeralexandrtalan/hbase"
    entrypoint: /bin/bash entrypoint.sh --command "slave" --config "/root/hbase-2.2.2/conf/hbase-config.json"
    restart: always
    volumes:
      - ./hbase-config:/root/hbase-2.2.2/conf
      - ./ssh_hbase:/root/.ssh/hbase
    ports:
      - "16030:16030"
    depends_on:
      - hbase_master
    networks:
      - hadoop
  hbase_region_server_2:
    container_name: hbase_region_server_2
    image: "docker.io/dockeralexandrtalan/hbase"
    entrypoint: /bin/bash entrypoint.sh --command "slave" --config "/root/hbase-2.2.2/conf/hbase-config.json"
    restart: always
    volumes:
      - ./hbase-config:/root/hbase-2.2.2/conf
      - ./ssh_hbase:/root/.ssh/hbase
    ports:
      - "16031:16030"
    depends_on:
      - hbase_master
    networks:
      - hadoop
  hbase_backup_master:
    container_name: hbase_backup_master
    image: "docker.io/dockeralexandrtalan/hbase"
    entrypoint: /bin/bash entrypoint.sh --command "slave" --config "/root/hbase-2.2.2/conf/hbase-config.json"
    restart: always
    volumes:
      - ./hbase-config:/root/hbase-2.2.2/conf
      - ./ssh_hbase:/root/.ssh/hbase
    ports:
      - "16011:16010"
    depends_on:
      - hbase_master
    networks:
      - hadoop


  crawler:
    container_name: crawler
    image: docker.io/dockeralexandrtalan/sbt
    volumes:
      - ./projects/Crawler:/root/crawler
    depends_on:
      - namenode
    networks:
      - hadoop


  spark_driver:
    container_name: spark_driver
    image: docker.io/dockeralexandrtalan/spark
    volumes:
      - ./projects/SparkApp:/root/spark/app
      - ./hadoop-config:/root/spark/hadoop/config
      - ./spark-config:/root/spark/spark-2.4.5-bin-hadoop2.7/conf/
    ports:
      - "4040:4040"
      - "18080:18080"
    environment:
      - HADOOP_CONF_DIR=/root/spark/hadoop/config
      - SPARK_DRIVER_JAR_PATH=/root/spark/app/target/scala-2.11
    depends_on:
      - resourcemanager
    networks:
      - hadoop

networks:
  hadoop: